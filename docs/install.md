# 1. Installation & Usage Guide

## âš™ï¸ Installation

**1. Prerequisites:**

- Python â‰¥ 3.9
- pip, git
- (Optional): DVC, MLflow, Streamlit, Airflow/Dagster, Docker

**2. Clone the repository:**

```bash
git clone https://github.com/your-org/retail-forecast-system.git
cd retail-forecast-system
```

**3. Create and activate virtual environment:**

```bash
python -m venv .venv
source .venv/bin/activate
```

On Windows:

```bash
.venv\Scripts\activate
```

**4. Install dependencies:**

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

**5. (Optional) Install extra tools:**

```bash
pip install dvc mlflow streamlit
```

**6. Set up `.env` file for credentials, DB, API keys**

```bash
cp .env-example .env
```

---

## ğŸ›  Usage

### 1ï¸âƒ£ **DVC Pipeline**

Run the **full pipeline** (from raw data to output, all stages auto-tracked):

```bash
dvc repro
```

- Runs every stage in `dvc.yaml` .
- Outputs, models, metrics, and plots are versioned and reproducible.

**Run a specific stage:**

```bash
dvc repro feature-engineering
dvc repro train-xgboost
```

**Visualize pipeline DAG:**

```bash
dvc dag
```

---

### 2ï¸âƒ£ **Dagster Pipeline**

**Run with Dagster (for orchestration, monitoring, and visualization):**

```bash
dagster dev
```

- Access Dagster UI at [http://localhost:3000](http://localhost:3000)
- Trigger jobs/assets , monitor status, debug runs.

**Example CLI:**

```bash
dagster job execute --job-name train_xgboost
dagster asset materialize --select feature_engineering
```

_Requires code mapping between pipeline stages and Dagster assets/jobs._

---

### 3ï¸âƒ£ **Python CLI / Typer**

**Manually run each stage** (for debugging, dev, or running small parts):

```bash
# Load data
python main.py load-data load-data --end-date 250630

# Preprocess: transform, cluster, filter
python main.py preprocess transform --input-file sales.csv --holiday-file holidays.csv --output-file transformed.csv
python main.py preprocess cluster-model --input-file transformed.csv --output-file data_clustered.csv --visualize
python main.py preprocess filter-data --input-file data_clustered.csv --output-file filtered.csv

# Feature engineering
python main.py feature-engineering feature-engineering --input-file filtered.csv --holiday-file holidays.csv --max-workers 8

# Train models (XGBoost/LightGBM/Prophet)
python main.py train batch-train --input-file train.csv --holiday-file holidays.csv --model-type xgboost --max-workers 8
python main.py train batch-train --input-file train.csv --holiday-file holidays.csv --model-type lightgbm --max-workers 8
python main.py train batch-train --input-file train.csv --model-type prophet --max-workers 8

# Predict
python main.py predict batch-predict --file-train train.csv --max-workers 8

# Reconcile (hierarchical forecast)
python main.py reconcile reconcile --input-file batch_forecast_results.csv --output-file reconciled_forecast_hierarchical.csv
```

---

### 4ï¸âƒ£ **Serving and Visualization**

- **API serving:**

  ```bash
  uvicorn api.main:app --reload
  ```

  Access Swagger UI at [http://localhost:8000/docs](http://localhost:8000/docs)

- **Dashboard (Streamlit):**

```bash
streamlit run app/app.py
```

![Workflow image](./images/dashboard_streamlit.png)

- **Experiment tracking (MLflow):**

  ```bash
  mlflow ui
  ```

  Open [http://localhost:5000](http://localhost:5000) for experiment history
  ![Workflow image](./images/mlflow_dashboard.png)
  ![Workflow image](./images/mlflow_compare.png)

- **Notebook EDA/prototyping:**

  ```bash
  jupyter notebook
  ```

---

### 5ï¸âƒ£ Documentation Site

#### **View docs:**

```bash
pip install mkdocs
mkdocs serve
```

---

# 2. Project Directory Structure

---

## ğŸš€ Features

- **Automated, reproducible ML pipeline** (data to deployment)
- **Advanced feature engineering** (time, lag, statistics, business, TSFresh, LLM-augmented)
- **AutoML and hyperparameter tuning** (Optuna/GridSearch)
- **Clustering for model specialization** (KMeans, GMM)
- **Flexible model selection:** Prophet, XGBoost, LightGBM, LLM-RAG
- **Multi-step and hierarchical forecasting:** recursive, direct, hybrid, reconciliation (Bottom-up, Top-down, Middle-out, OLS, MinTrace)
- **Explainability:** SHAP, LLM auto-diagnosis, reporting
- **MLOps:** MLflow (metrics, models), DVC (data versioning), Airflow/Dagster (orchestration), FastAPI (serving), Streamlit (visualization)
- **Batch & real-time forecasting**
- **Designed for extensibility:** easily add new models, features, or data sources

---

## ğŸ“ Project Structure

```text
retail-forecast-system/
â”‚
â”œâ”€â”€ airflow/                  # Airflow DAGs for orchestrating data pipelines and batch workflows
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ system_retail_pipeline.py   # Main Airflow DAG for retail system pipeline automation
â”‚
â”œâ”€â”€ api/                      # FastAPI app â€“ RESTful API endpoints for inference, health check, etc.
â”‚   â”œâ”€â”€ endpoints.py          # API route handlers
â”‚   â”œâ”€â”€ main.py               # API app entrypoint
â”‚   â”œâ”€â”€ schema.py             # Pydantic schemas for request/response validation
â”‚   â””â”€â”€ utils.py              # API-specific utilities
â”‚
â”œâ”€â”€ app/                      # Lightweight app front-end or dashboard (e.g. Streamlit, Gradio, etc.)
â”‚   â”œâ”€â”€ pages/                # Individual app pages (e.g., EDA, visualization)
â”‚   â”‚   â””â”€â”€ EDA.PY            # Exploratory Data Analysis dashboard page
â”‚   â”œâ”€â”€ vectordb/             # Vector DB index files (for embedding, semantic search, etc.)
â”‚   â”‚   â””â”€â”€ faiss.index       # FAISS index for fast vector search
â”‚   â”œâ”€â”€ README.md             # App-specific README/instructions
â”‚   â””â”€â”€ app.py                # Main app entrypoint
â”‚
â”œâ”€â”€ config/                   # YAML configuration files for parameters, models, paths
â”‚   â”œâ”€â”€ config.yaml           # Global project config (paths, globals)
â”‚   â””â”€â”€ models.yaml           # Model-specific hyperparameter config
â”‚
â”œâ”€â”€ dagster_pipeline/         # Dagster pipeline definitions (alternative or complement to Airflow)
â”‚   â”œâ”€â”€ assets.py             # Dagster asset definitions
â”‚   â”œâ”€â”€ dagster.yaml          # Dagster config
â”‚   â”œâ”€â”€ jobs.py               # Job orchestration (Dagster)
â”‚   â””â”€â”€ schedules.py          # Scheduled jobs (Dagster)
â”‚
â”œâ”€â”€ data/                     # Data storage (DO NOT track raw here if using DVC!)
â”‚   â”œâ”€â”€ feature-store/        # Feature store for ML training/inference
â”‚   â”œâ”€â”€ interim/              # Intermediate, cleaned, or transformed data for experiments
â”‚   â”œâ”€â”€ output/               # Output from batch predictions, evaluations, etc.
â”‚   â”œâ”€â”€ processed/            # Final/production-ready datasets
â”‚   â”œâ”€â”€ raw/                  # Immutable raw source data (never edit manually)
â”‚   â””â”€â”€ sql/                  # SQL scripts for ETL or data exploration
â”‚
â”œâ”€â”€ docs/                     # Project documentation (Markdown + site output for mkdocs)
â”‚   â”œâ”€â”€ docs/                 # Main docs in Markdown, organized by topic
â”‚   â”œâ”€â”€ .gitkeep              # Ensures folder in git if empty
â”‚   â”œâ”€â”€ README.md             # Documentation for documentation! (Contributing, etc.)
â”‚   â””â”€â”€ mkdocs.yml            # mkdocs configuration for static site generator
â”‚
â”œâ”€â”€ mlruns/                   # MLflow experiment tracking (auto-created, ignore unless using MLflow)
â”‚
â”œâ”€â”€ models/                   # Serialized trained models, artifacts, and checkpoints
â”‚   â””â”€â”€ .gitkeep              # Keep empty directory tracked by git
â”‚
â”œâ”€â”€ notebooks/                # Jupyter/Colab notebooks (experimentation, prototyping, reports)
â”‚   â”œâ”€â”€ .gitkeep              # Empty dir marker
â”‚   â”œâ”€â”€ [1]_EDA.ipynb         # Exploratory Data Analysis notebook
â”‚   â”œâ”€â”€ [2]_Report.ipynb      # Project summary report notebook
â”‚   â””â”€â”€ [3]_Pipeline-model.ipynb  # Modeling/ML pipeline notebook
â”‚
â”œâ”€â”€ rag/                      # Retrieval-Augmented Generation (RAG) code and configs
â”‚   â”œâ”€â”€ prompts/              # Prompt templates for LLM-driven RAG workflows
â”‚   â”‚   â””â”€â”€ file_analysis.yaml# RAG prompt for file analysis
â”‚   â”œâ”€â”€ chunking.py           # Text chunking utilities for RAG
â”‚   â”œâ”€â”€ data_loader.py        # Data loading utilities for RAG
â”‚   â”œâ”€â”€ embedding.py          # Embedding functions/classes
â”‚   â”œâ”€â”€ llm.py                # LLM interface
â”‚   â”œâ”€â”€ pipeline.py           # End-to-end RAG pipeline definition
â”‚   â”œâ”€â”€ prompt_engine.py      # Prompt engineering logic
â”‚   â”œâ”€â”€ reranker.py           # Result reranking logic
â”‚   â”œâ”€â”€ retriever.py          # Retrieval utilities
â”‚   â””â”€â”€ vectordb.py           # Vector database wrapper
â”‚
â”œâ”€â”€ reports/                  # Generated reports, figures, and output for business/tech
â”‚   â”œâ”€â”€ figures/              # Visual assets, plots, images for reporting
â”‚   â””â”€â”€ .gitkeep              # Keep empty directory in git
â”‚
â”œâ”€â”€ src/                      # Core source code: ETL, ML, feature engineering, modeling, etc.
â”‚   â”œâ”€â”€ db/                   # Database connection and pipeline modules
â”‚   â”œâ”€â”€ features/             # Automated/manual feature engineering scripts
â”‚   â”œâ”€â”€ modeling/             # ML/DL model classes and logic (XGBoost, LSTM, Prophet, etc.)
â”‚   â”œâ”€â”€ strategies/           # Forecasting strategies: benchmark, multi-step, etc.
â”‚   â”œâ”€â”€ utils/                # General-purpose utility functions
â”‚   â”œâ”€â”€ config.py             # Centralized configuration loader/manager
â”‚   â”œâ”€â”€ cross_validator.py    # Custom cross-validation logic for time series
â”‚   â”œâ”€â”€ feature.py            # Feature store logic/integration
â”‚   â”œâ”€â”€ forecast_pipeline.py  # End-to-end forecast pipeline logic
â”‚   â”œâ”€â”€ load_data.py          # Data loading and parsing logic
â”‚   â”œâ”€â”€ plots.py              # Visualization utilities for model results
â”‚   â”œâ”€â”€ predict.py            # Inference logic for predictions
â”‚   â”œâ”€â”€ preprocess.py         # Data preprocessing/cleaning logic
â”‚   â”œâ”€â”€ reconcile.py          # Hierarchical reconciliation logic
â”‚   â””â”€â”€ train.py              # Training orchestration/entrypoint
â”‚
â”œâ”€â”€ .dvcignore                # DVC ignore patterns for data version control
â”œâ”€â”€ .env                      # Environment variables (never commit secrets!)
â”œâ”€â”€ Dockerfile                # Docker image definition for reproducible builds/deployment
â”œâ”€â”€ Makefile                  # Automate project workflows (train, test, build, lint, etc.)
â”œâ”€â”€ README.md                 # Project overview, usage, and instructions (first file people read)
â”œâ”€â”€ docker-compose.yaml       # Multi-container Docker orchestration (dev/test/prod)
â”œâ”€â”€ dvc.lock                  # DVC pipeline lock file (auto-generated)
â”œâ”€â”€ dvc.yaml                  # DVC pipeline config for data versioning
â”œâ”€â”€ install_ray.bat           # Script to install Ray for distributed processing
â”œâ”€â”€ main.py                   # Project CLI/main entrypoint (run pipelines, apps, etc.)
â”œâ”€â”€ pyproject.toml            # Python project metadata/build config (PEP 518+)
â””â”€â”€ requirements.txt          # Python package dependencies (always pin versions for reproducibility)

```
