{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Forecasting System Documentation","text":"<p>Welcome to the documentation for the Forecasting System.</p> <p>This project provides a scalable and automated time series forecasting pipeline designed for real-time and batch use cases, with model selection, data clustering, and hierarchical reconciliation.</p> <p>Use the sidebar to explore documentation sections including models used, pipeline structure, deployment workflow, and technical appendices.</p>"},{"location":"#project-overview","title":"Project Overview","text":"<p>This project aims to develop a sales forecasting system, a large-scale retailer with numerous stores distributed across multiple geographic regions. The forecasting model is designed to capture the unique characteristics of each store and product group - across five main product categories: Fresh Food, Packaged Food, Home &amp; Personal Care, Household Items, and Apparel - as well as regional differences across locations.</p> <p>The entire pipeline is fully automated following the standard machine learning lifecycle, including data ingestion, preprocessing, feature engineering, model training and evaluation and deployment. Forecasts are generated in both batch and real-time formats. Time series data is thoroughly analyzed using features such as trend, seasonality and volatility, followed by KMeans clustering to optimize forecast accuracy for different store and product group profiles.</p> <p>The system integrates several tools to ensure reliability, reusability and scalability: DVC for data version control, MLflow for model lifecycle management, FastAPI for serving prediction APIs, Airflow for orchestration of data processing workflows, Slack notifications for real-time monitoring and alerts and a Data Pipeline framework to manage end-to-end processing.</p> <p>This solution enables the business team to better plan sales, allocate resources more efficiently, and make data-driven decisions quickly, by leveraging both historical data and future projections.</p>"},{"location":"architecture/","title":"6. Forecasting Pipeline","text":"<p><p>Machine Learning Forecasting Pipeline Diagram</p></p> <p>The forecasting pipeline is built as a modular, scalable architecture to maximize reusability, flexibility, and robust deployment in production settings. Each step below corresponds to a key building block in the end-to-end ML process:</p>"},{"location":"architecture/#1-data-ingestion","title":"1. Data Ingestion","text":"<ul> <li>Objective: Aggregate data from multiple sources and standardize into a unified schema (typically columns like <code>ds</code>, <code>y</code>, plus regressors).</li> <li>Why: Ensures compatibility across models and tools; enables seamless integration with upstream data sources.</li> </ul>"},{"location":"architecture/#2-preprocessing","title":"2. Preprocessing","text":"<ul> <li>Objective: Clean and prepare raw data.</li> <li>Key tasks: Date format normalization, handling missing values/outliers, adjusting data granularity (e.g., by store, product group).</li> <li>Impact: Reduces noise, improves model robustness and data quality.</li> </ul>"},{"location":"architecture/#3-feature-engineering","title":"3. Feature Engineering","text":"<ul> <li>Automated (TSFresh): Extracts Fourier coefficients, entropy, statistical and temporal features.</li> <li>Manual: Incorporates business-driven or domain-specific variables.</li> <li>Hybrid (Prophet): Leverages decomposed trend, seasonality, special event features.</li> <li>LLM-RAG: Uses generative AI for feature suggestion, automated reports, and advanced diagnostics.</li> </ul>"},{"location":"architecture/#4-feature-selection","title":"4. Feature Selection","text":"<ul> <li>Objective: Identify and retain the most predictive features.</li> <li>Tools: AutoML routines, SHAP for feature importance, LLM-RAG for diagnostic summaries and data quality checks.</li> </ul>"},{"location":"architecture/#5-model-training","title":"5. Model Training","text":"<ul> <li>Objective: Fit the best forecasting model(s) to each data group or cluster.</li> <li> <p>Strategy: Clustering (e.g., KMeans) assigns time series to groups.</p> </li> <li> <p>Prophet: For series with strong trend/seasonality.</p> </li> <li>XGBoost: For nonlinear, complex signals.</li> <li>LightGBM: For noisy, large-scale or highly variable data.</li> </ul>"},{"location":"architecture/#6-hyperparameter-tuning","title":"6. Hyperparameter Tuning","text":"<ul> <li>Objective: Find the optimal model hyperparameters.</li> <li>Method: Leverage Optuna or Grid Search per group/cluster, supports both global and per-model tuning.</li> </ul>"},{"location":"architecture/#7-cross-validation","title":"7. Cross-Validation","text":"<ul> <li>Objective: Robustly estimate generalization error and prevent overfitting.</li> <li>How: Time series cross-validation splits (respects data order), multiple train/test splits, supports metric-based model selection.</li> </ul>"},{"location":"architecture/#8-model-evaluation","title":"8. Model Evaluation","text":"<ul> <li>Objective: Quantify and compare model performance.</li> <li>Metrics: MAE, RMSE, MAPE, SMAPE, Coverage, etc.</li> <li>Tools: MLflow logs all metrics, model artifacts, and visualizations.</li> </ul>"},{"location":"architecture/#9-forecast-reconciliation","title":"9. Forecast Reconciliation","text":"<ul> <li>Objective: Ensure coherence and consistency in multi-level (hierarchical) forecasts.</li> <li>How: Use <code>HierarchicalForecast</code> with strategies like Bottom-up, Top-down, OLS, MinTrace to enforce aggregation constraints (e.g., system &gt; region &gt; store &gt; SKU).</li> </ul>"},{"location":"architecture/#10-model-diagnosis","title":"10. Model Diagnosis","text":"<ul> <li>Objective: Monitor training behavior and diagnose issues.</li> <li>Tasks: Plot learning curves, inspect train/val loss, learning rate schedules, spot overfitting/underfitting, flag anomalies.</li> </ul>"},{"location":"architecture/#11-deployment","title":"11. Deployment","text":"<ul> <li>Objective: Serve models for batch or real-time forecasting.</li> <li>Tools: FastAPI provides REST API endpoints; supports both scheduled jobs and on-demand requests.</li> </ul>"},{"location":"architecture/#12-prediction-output","title":"12. Prediction Output","text":"<ul> <li>Deliverables: Forecasts (<code>yhat</code>, <code>yhat_lower</code>, <code>yhat_upper</code>), actuals (<code>y</code>), and time index (<code>ds</code>).</li> <li>Usage: Feeds into planning, downstream systems, and performance evaluation.</li> </ul>"},{"location":"architecture/#13-visualization-reporting","title":"13. Visualization &amp; Reporting","text":"<ul> <li>Objective: Enable business and technical users to interact with results.</li> <li>Tools: Streamlit dashboard for trend analysis, forecast vs. actual comparison, error breakdowns, and auto-generated reports.</li> </ul> <p>This modular pipeline enables the system to scale, adapt to changing business requirements, and ensure each step is traceable, testable, and easily maintained.</p> <p>For a visual summary, see the Workflow Diagram above. For deeper technical detail on each module, refer to the Architecture and Feature Engineering sections.</p>"},{"location":"feature_engineering/","title":"3. Feature Engineering","text":""},{"location":"feature_engineering/#business-feature","title":"Business Feature","text":"<p>Business Features are categorical or numeric variables that reflect domain-specific business behaviors, enhancing the model's interpretability and forecasting accuracy. In this project, key Business Features include: </p> Business Feature Description Purpose / Impact Wholesale Sales from wholesale customers often exhibit high volume but unstable patterns. These transactions may influence retail customer purchasing behavior. Captures volatility from wholesale segment; accounts for its effect on more stable retail sales Visits Represents the number of customer visits per day or time unit. Reflects store performance and customer behavior over time. Helps model demand fluctuation based on shopper volume; supports sales variation and performance analysis"},{"location":"feature_engineering/#time-feature","title":"Time Feature","text":"<p>Time features are extracted from the date column (ds) to describe calendar-based cycles, seasonality and trends. These features enable the model to capture recurring temporal patterns, such as:</p> Time Feature Description Day of the week Monday, Tuesday, etc. Month of the year January, February, etc. Week of the year Week 1, Week 2, etc. Holidays Tet, Christmas, Black Friday, etc. Fourier coefficients Encoded periodic patterns in time series using sine and cosine transforms"},{"location":"feature_engineering/#lag-feature","title":"Lag Feature","text":"<p>Lag features are sales values shifted back by a specific time period, derived from past values of the target variable (sales) or related variables. In this project, lag features play a key role in enabling the model to learn autoregressive relationships \u2014 a crucial factor in time series forecasting. However, the value of a lag feature depends on the point in the forecasting horizon you are predicting. For example, sales on day D can be used as:</p> <ul> <li>1-day lag when forecasting D+1,</li> <li>2-day lag when forecasting D+2,</li> <li>7-day lag when forecasting D+7,</li> <li>14-day lag when forecasting D+14,</li> <li>28-day lag when forecasting D+28.</li> </ul> <p></p>"},{"location":"feature_engineering/#auto-statistic-feature-tsfresh","title":"Auto statistic Feature (TSFresh)","text":"<p>Time series features are automatically extracted using the TSFresh library, which generates a wide variety of statistical and signal-based features, including: </p> Auto statistic Feature (TSFresh) Description Energy and Entropy Measure signal strength and randomness of the time series. Mean, Variance, Skewness, Kurtosis Describe central tendency, dispersion, and shape of distribution. Autocorrelation and Partial Autocorrelation Measure relationships between current and past values."},{"location":"install/","title":"1. Installation &amp; Usage Guide","text":""},{"location":"install/#installation","title":"\u2699\ufe0f Installation","text":"<p>1. Prerequisites:</p> <ul> <li>Python \u2265 3.9</li> <li>pip, git</li> <li>(Optional): DVC, MLflow, Streamlit, Airflow/Dagster, Docker</li> </ul> <p>2. Clone the repository:</p> <pre><code>git clone https://github.com/your-org/retail-forecast-system.git\ncd retail-forecast-system\n</code></pre> <p>3. Create and activate virtual environment:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre> <p>On Windows:</p> <pre><code>.venv\\Scripts\\activate\n</code></pre> <p>4. Install dependencies:</p> <pre><code>pip install --upgrade pip\npip install -r requirements.txt\n</code></pre> <p>5. (Optional) Install extra tools:</p> <pre><code>pip install dvc mlflow streamlit\n</code></pre> <p>6. Set up <code>.env</code> file for credentials, DB, API keys</p> <pre><code>cp .env-example .env\n</code></pre>"},{"location":"install/#usage","title":"\ud83d\udee0 Usage","text":""},{"location":"install/#1-dvc-pipeline","title":"1\ufe0f\u20e3 DVC Pipeline","text":"<p>Run the full pipeline (from raw data to output, all stages auto-tracked):</p> <pre><code>dvc repro\n</code></pre> <ul> <li>Runs every stage in <code>dvc.yaml</code> .</li> <li>Outputs, models, metrics, and plots are versioned and reproducible.</li> </ul> <p>Run a specific stage:</p> <pre><code>dvc repro feature-engineering\ndvc repro train-xgboost\n</code></pre> <p>Visualize pipeline DAG:</p> <pre><code>dvc dag\n</code></pre>"},{"location":"install/#2-dagster-pipeline","title":"2\ufe0f\u20e3 Dagster Pipeline","text":"<p>Run with Dagster (for orchestration, monitoring, and visualization):</p> <pre><code>dagster dev\n</code></pre> <ul> <li>Access Dagster UI at http://localhost:3000</li> <li>Trigger jobs/assets , monitor status, debug runs.</li> </ul> <p>Example CLI:</p> <pre><code>dagster job execute --job-name train_xgboost\ndagster asset materialize --select feature_engineering\n</code></pre> <p>Requires code mapping between pipeline stages and Dagster assets/jobs.</p>"},{"location":"install/#3-python-cli-typer","title":"3\ufe0f\u20e3 Python CLI / Typer","text":"<p>Manually run each stage (for debugging, dev, or running small parts):</p> <pre><code># Load data\npython main.py load-data load-data --end-date 250630\n\n# Preprocess: transform, cluster, filter\npython main.py preprocess transform --input-file sales.csv --holiday-file holidays.csv --output-file transformed.csv\npython main.py preprocess cluster-model --input-file transformed.csv --output-file data_clustered.csv --visualize\npython main.py preprocess filter-data --input-file data_clustered.csv --output-file filtered.csv\n\n# Feature engineering\npython main.py feature-engineering feature-engineering --input-file filtered.csv --holiday-file holidays.csv --max-workers 8\n\n# Train models (XGBoost/LightGBM/Prophet)\npython main.py train batch-train --input-file train.csv --holiday-file holidays.csv --model-type xgboost --max-workers 8\npython main.py train batch-train --input-file train.csv --holiday-file holidays.csv --model-type lightgbm --max-workers 8\npython main.py train batch-train --input-file train.csv --model-type prophet --max-workers 8\n\n# Predict\npython main.py predict batch-predict --file-train train.csv --max-workers 8\n\n# Reconcile (hierarchical forecast)\npython main.py reconcile reconcile --input-file batch_forecast_results.csv --output-file reconciled_forecast_hierarchical.csv\n</code></pre>"},{"location":"install/#4-serving-and-visualization","title":"4\ufe0f\u20e3 Serving and Visualization","text":"<ul> <li>API serving:</li> </ul> <pre><code>uvicorn api.main:app --reload\n</code></pre> <p>Access Swagger UI at http://localhost:8000/docs</p> <ul> <li>Dashboard (Streamlit):</li> </ul> <pre><code>streamlit run app/app.py\n</code></pre> <p></p> <ul> <li>Experiment tracking (MLflow):</li> </ul> <pre><code>mlflow ui\n</code></pre> <p>Open http://localhost:5000 for experiment history    </p> <ul> <li>Notebook EDA/prototyping:</li> </ul> <pre><code>jupyter notebook\n</code></pre>"},{"location":"install/#5-documentation-site","title":"5\ufe0f\u20e3 Documentation Site","text":""},{"location":"install/#view-docs","title":"View docs:","text":"<pre><code>pip install mkdocs\nmkdocs serve\n</code></pre>"},{"location":"install/#2-project-directory-structure","title":"2. Project Directory Structure","text":""},{"location":"install/#features","title":"\ud83d\ude80 Features","text":"<ul> <li>Automated, reproducible ML pipeline (data to deployment)</li> <li>Advanced feature engineering (time, lag, statistics, business, TSFresh, LLM-augmented)</li> <li>AutoML and hyperparameter tuning (Optuna/GridSearch)</li> <li>Clustering for model specialization (KMeans, GMM)</li> <li>Flexible model selection: Prophet, XGBoost, LightGBM, LLM-RAG</li> <li>Multi-step and hierarchical forecasting: recursive, direct, hybrid, reconciliation (Bottom-up, Top-down, Middle-out, OLS, MinTrace)</li> <li>Explainability: SHAP, LLM auto-diagnosis, reporting</li> <li>MLOps: MLflow (metrics, models), DVC (data versioning), Airflow/Dagster (orchestration), FastAPI (serving), Streamlit (visualization)</li> <li>Batch &amp; real-time forecasting</li> <li>Designed for extensibility: easily add new models, features, or data sources</li> </ul>"},{"location":"install/#project-structure","title":"\ud83d\udcc1 Project Structure","text":"<pre><code>retail-forecast-system/\n\u2502\n\u251c\u2500\u2500 airflow/                  # Airflow DAGs for orchestrating data pipelines and batch workflows\n\u2502   \u2514\u2500\u2500 dags/\n\u2502       \u2514\u2500\u2500 system_retail_pipeline.py   # Main Airflow DAG for retail system pipeline automation\n\u2502\n\u251c\u2500\u2500 api/                      # FastAPI app \u2013 RESTful API endpoints for inference, health check, etc.\n\u2502   \u251c\u2500\u2500 endpoints.py          # API route handlers\n\u2502   \u251c\u2500\u2500 main.py               # API app entrypoint\n\u2502   \u251c\u2500\u2500 schema.py             # Pydantic schemas for request/response validation\n\u2502   \u2514\u2500\u2500 utils.py              # API-specific utilities\n\u2502\n\u251c\u2500\u2500 app/                      # Lightweight app front-end or dashboard (e.g. Streamlit, Gradio, etc.)\n\u2502   \u251c\u2500\u2500 pages/                # Individual app pages (e.g., EDA, visualization)\n\u2502   \u2502   \u2514\u2500\u2500 EDA.PY            # Exploratory Data Analysis dashboard page\n\u2502   \u251c\u2500\u2500 vectordb/             # Vector DB index files (for embedding, semantic search, etc.)\n\u2502   \u2502   \u2514\u2500\u2500 faiss.index       # FAISS index for fast vector search\n\u2502   \u251c\u2500\u2500 README.md             # App-specific README/instructions\n\u2502   \u2514\u2500\u2500 app.py                # Main app entrypoint\n\u2502\n\u251c\u2500\u2500 config/                   # YAML configuration files for parameters, models, paths\n\u2502   \u251c\u2500\u2500 config.yaml           # Global project config (paths, globals)\n\u2502   \u2514\u2500\u2500 models.yaml           # Model-specific hyperparameter config\n\u2502\n\u251c\u2500\u2500 dagster_pipeline/         # Dagster pipeline definitions (alternative or complement to Airflow)\n\u2502   \u251c\u2500\u2500 assets.py             # Dagster asset definitions\n\u2502   \u251c\u2500\u2500 dagster.yaml          # Dagster config\n\u2502   \u251c\u2500\u2500 jobs.py               # Job orchestration (Dagster)\n\u2502   \u2514\u2500\u2500 schedules.py          # Scheduled jobs (Dagster)\n\u2502\n\u251c\u2500\u2500 data/                     # Data storage (DO NOT track raw here if using DVC!)\n\u2502   \u251c\u2500\u2500 feature-store/        # Feature store for ML training/inference\n\u2502   \u251c\u2500\u2500 interim/              # Intermediate, cleaned, or transformed data for experiments\n\u2502   \u251c\u2500\u2500 output/               # Output from batch predictions, evaluations, etc.\n\u2502   \u251c\u2500\u2500 processed/            # Final/production-ready datasets\n\u2502   \u251c\u2500\u2500 raw/                  # Immutable raw source data (never edit manually)\n\u2502   \u2514\u2500\u2500 sql/                  # SQL scripts for ETL or data exploration\n\u2502\n\u251c\u2500\u2500 docs/                     # Project documentation (Markdown + site output for mkdocs)\n\u2502   \u251c\u2500\u2500 docs/                 # Main docs in Markdown, organized by topic\n\u2502   \u251c\u2500\u2500 .gitkeep              # Ensures folder in git if empty\n\u2502   \u251c\u2500\u2500 README.md             # Documentation for documentation! (Contributing, etc.)\n\u2502   \u2514\u2500\u2500 mkdocs.yml            # mkdocs configuration for static site generator\n\u2502\n\u251c\u2500\u2500 mlruns/                   # MLflow experiment tracking (auto-created, ignore unless using MLflow)\n\u2502\n\u251c\u2500\u2500 models/                   # Serialized trained models, artifacts, and checkpoints\n\u2502   \u2514\u2500\u2500 .gitkeep              # Keep empty directory tracked by git\n\u2502\n\u251c\u2500\u2500 notebooks/                # Jupyter/Colab notebooks (experimentation, prototyping, reports)\n\u2502   \u251c\u2500\u2500 .gitkeep              # Empty dir marker\n\u2502   \u251c\u2500\u2500 [1]_EDA.ipynb         # Exploratory Data Analysis notebook\n\u2502   \u251c\u2500\u2500 [2]_Report.ipynb      # Project summary report notebook\n\u2502   \u2514\u2500\u2500 [3]_Pipeline-model.ipynb  # Modeling/ML pipeline notebook\n\u2502\n\u251c\u2500\u2500 rag/                      # Retrieval-Augmented Generation (RAG) code and configs\n\u2502   \u251c\u2500\u2500 prompts/              # Prompt templates for LLM-driven RAG workflows\n\u2502   \u2502   \u2514\u2500\u2500 file_analysis.yaml# RAG prompt for file analysis\n\u2502   \u251c\u2500\u2500 chunking.py           # Text chunking utilities for RAG\n\u2502   \u251c\u2500\u2500 data_loader.py        # Data loading utilities for RAG\n\u2502   \u251c\u2500\u2500 embedding.py          # Embedding functions/classes\n\u2502   \u251c\u2500\u2500 llm.py                # LLM interface\n\u2502   \u251c\u2500\u2500 pipeline.py           # End-to-end RAG pipeline definition\n\u2502   \u251c\u2500\u2500 prompt_engine.py      # Prompt engineering logic\n\u2502   \u251c\u2500\u2500 reranker.py           # Result reranking logic\n\u2502   \u251c\u2500\u2500 retriever.py          # Retrieval utilities\n\u2502   \u2514\u2500\u2500 vectordb.py           # Vector database wrapper\n\u2502\n\u251c\u2500\u2500 reports/                  # Generated reports, figures, and output for business/tech\n\u2502   \u251c\u2500\u2500 figures/              # Visual assets, plots, images for reporting\n\u2502   \u2514\u2500\u2500 .gitkeep              # Keep empty directory in git\n\u2502\n\u251c\u2500\u2500 src/                      # Core source code: ETL, ML, feature engineering, modeling, etc.\n\u2502   \u251c\u2500\u2500 db/                   # Database connection and pipeline modules\n\u2502   \u251c\u2500\u2500 features/             # Automated/manual feature engineering scripts\n\u2502   \u251c\u2500\u2500 modeling/             # ML/DL model classes and logic (XGBoost, LSTM, Prophet, etc.)\n\u2502   \u251c\u2500\u2500 strategies/           # Forecasting strategies: benchmark, multi-step, etc.\n\u2502   \u251c\u2500\u2500 utils/                # General-purpose utility functions\n\u2502   \u251c\u2500\u2500 config.py             # Centralized configuration loader/manager\n\u2502   \u251c\u2500\u2500 cross_validator.py    # Custom cross-validation logic for time series\n\u2502   \u251c\u2500\u2500 feature.py            # Feature store logic/integration\n\u2502   \u251c\u2500\u2500 forecast_pipeline.py  # End-to-end forecast pipeline logic\n\u2502   \u251c\u2500\u2500 load_data.py          # Data loading and parsing logic\n\u2502   \u251c\u2500\u2500 plots.py              # Visualization utilities for model results\n\u2502   \u251c\u2500\u2500 predict.py            # Inference logic for predictions\n\u2502   \u251c\u2500\u2500 preprocess.py         # Data preprocessing/cleaning logic\n\u2502   \u251c\u2500\u2500 reconcile.py          # Hierarchical reconciliation logic\n\u2502   \u2514\u2500\u2500 train.py              # Training orchestration/entrypoint\n\u2502\n\u251c\u2500\u2500 .dvcignore                # DVC ignore patterns for data version control\n\u251c\u2500\u2500 .env                      # Environment variables (never commit secrets!)\n\u251c\u2500\u2500 Dockerfile                # Docker image definition for reproducible builds/deployment\n\u251c\u2500\u2500 Makefile                  # Automate project workflows (train, test, build, lint, etc.)\n\u251c\u2500\u2500 README.md                 # Project overview, usage, and instructions (first file people read)\n\u251c\u2500\u2500 docker-compose.yaml       # Multi-container Docker orchestration (dev/test/prod)\n\u251c\u2500\u2500 dvc.lock                  # DVC pipeline lock file (auto-generated)\n\u251c\u2500\u2500 dvc.yaml                  # DVC pipeline config for data versioning\n\u251c\u2500\u2500 install_ray.bat           # Script to install Ray for distributed processing\n\u251c\u2500\u2500 main.py                   # Project CLI/main entrypoint (run pipelines, apps, etc.)\n\u251c\u2500\u2500 pyproject.toml            # Python project metadata/build config (PEP 518+)\n\u2514\u2500\u2500 requirements.txt          # Python package dependencies (always pin versions for reproducibility)\n</code></pre>"},{"location":"models/","title":"4. Model Characteristics","text":"<p>The platform integrates a diverse set of algorithms, enabling robust, scalable, and explainable sales forecasting. Models and strategies are automatically selected and tuned based on data characteristics, business requirements, and prediction horizon. Below are the main modeling approaches used in the system:</p>"},{"location":"models/#a-unsupervised-clustering-kmeans-for-time-series-segmentation","title":"A. Unsupervised Clustering: KMeans for Time Series Segmentation","text":"<p>KMeans clustering groups time series with similar patterns, allowing the system to assign the most suitable forecasting model for each group (e.g., XGBoost, Prophet). This tailored approach increases overall accuracy, handles heterogeneity across stores/SKUs, and enables custom strategies at multiple hierarchy levels.</p>"},{"location":"models/#key-features-used-for-clustering","title":"Key Features Used for Clustering:","text":"Feature Description Length Series duration (# periods with data) Sparsity Proportion of zero sales Coefficient of Variation (CV) Standard deviation / mean, measuring relative variability Trend Strength Degree and direction of trend over time Seasonality Strength of repeating patterns Skewness Asymmetry in distribution Kurtosis Peakedness, tail heaviness/outlier tendency Entropy Randomness or predictability Number of Peaks Local maxima count (volatility indicator) AFC (Auto-regressive Feature Correlation) Short-term dependence on previous values ACF Mean Mean autocorrelation at short lags Dominant Frequency Main repeating cycle detected Rolling Slope Average trend over rolling windows <p>After clustering, the pipeline automatically assigns and tunes the best model for each group, adapting to changes in data hierarchy or quality.</p>"},{"location":"models/#b-core-forecasting-models","title":"B. Core Forecasting Models","text":"Model Type Strengths Use Case in Retail Forecasting XGBoost Gradient Boosted Trees Handles complex, non-linear tabular data; robust to noise; interpretable; fast training; excellent for short-term Adapts to store/SKU-specific factors, promotions, sudden changes Prophet Decomposable Time Series Captures trend, seasonality, holiday effects; interpretable components; best for regular, cyclic sales patterns Forecasts seasonal demand, marketing/holiday impacts, business cycles LightGBM Fast Boosted Trees Optimized for large-scale, high-dimensional, or noisy data; less sensitive to missing patterns Effective for unstable, highly variable multi-store/product data <ul> <li>Model assignment is automated: Based on cluster profiles, data diagnostics, and business priorities.</li> </ul>"},{"location":"models/#c-generative-ai-llm-rag-retrieval-augmented-generation","title":"C. Generative AI: LLM RAG (Retrieval-Augmented Generation)","text":"<p>LLM RAG (Retrieval-Augmented Generation) brings GenAI directly into the forecasting pipeline for complex, highly non-stationary, or outlier-prone time series.</p> <p></p>"},{"location":"models/#how-rag-is-used","title":"How RAG is Used:","text":"<ol> <li> <p>Build a Knowledge Base:</p> </li> <li> <p>Historical time series are segmented, featurized, and clustered (e.g. KMeans).</p> </li> <li> <p>Representative patterns are stored as a reference database.</p> </li> <li> <p>Retrieve and Prompt:</p> </li> <li> <p>For a new input, the system finds the top-K most similar sequences (using DTW, clustering, or embeddings).</p> </li> <li> <p>The input and reference sequences are formatted as a natural language prompt with clear instructions for the LLM.</p> </li> <li> <p>LLM-Driven Forecasting &amp; Diagnosis:</p> </li> <li> <p>The LLM uses both input data and retrieved patterns to generate forecasts, detect anomalies, or explain uncertainty.</p> </li> <li>This augments statistical and ML models with external, context-rich knowledge\u2014without requiring model fine-tuning.</li> </ol>"},{"location":"models/#benefits-of-llm-rag-integration","title":"Benefits of LLM RAG Integration","text":"<ul> <li>Enhances accuracy and stability for volatile or rare-event time series.</li> <li>Automates feature suggestion and selection (knowledge-augmented FE).</li> <li>Provides instant diagnostic and explainability reports for business users and data scientists.</li> <li>Reduces hallucination and adapts across new domains (no extra retraining).</li> <li>Supports anomaly detection and knowledge-based scenario generation.</li> </ul> <p>The synergy between classic ML, statistical forecasting, and GenAI makes the system highly flexible, resilient, and future-proof for modern retail operations.</p>"},{"location":"strategies/","title":"7. Strategies","text":""},{"location":"strategies/#multi-step-forecasting-strategies","title":"Multi-Step Forecasting Strategies","text":"<p>Multi-step forecasting predicts a sequence of future values (e.g., next 7/30 days of sales). There are three core strategies: Direct, Recursive, and Hybrid.</p>"},{"location":"strategies/#1-direct-multi-step-forecasting","title":"1\ufe0f\u20e3 Direct Multi-step Forecasting","text":"<p>Definition: A separate model is trained for each forecast horizon (T+1, T+2, ..., T+H).</p> <p>Advantages</p> <ul> <li>\ud83d\udfe2 No error accumulation: Each step\u2019s forecast is independent.</li> <li>\ud83d\udee0\ufe0f Step-specific tuning: Optimize features/model for each horizon.</li> <li>\ud83c\udfaf Superior for long horizons: Accuracy stays high even many steps ahead.</li> </ul> <p>Disadvantages</p> <ul> <li>\ud83d\udd34 Complexity: Needs H models (one per horizon).</li> <li>\ud83d\udcb8 Resource intensive: More compute, more maintenance.</li> <li>\u26a0\ufe0f Framework limitations: Not all ML libraries support multi-output.</li> </ul> <p></p> <p>When to use:</p> <ul> <li>You need top accuracy for each future step (e.g., daily business KPIs).</li> <li>Have enough data and compute for multi-model training.</li> </ul>"},{"location":"strategies/#2-recursive-multi-step-forecasting","title":"2\ufe0f\u20e3 Recursive Multi-step Forecasting","text":"<p>Definition: Train a single model for one-step-ahead. Predict recursively: use each output as next input.</p> <p>Advantages</p> <ul> <li>\ud83d\udfe2 Simplicity: Only one model to train, deploy, maintain.</li> <li>\ud83e\udeb6 Lightweight: Low resource requirements.</li> <li>\u26a1 Fast updates: Good for rapid retraining.</li> </ul> <p>Disadvantages</p> <ul> <li>\ud83d\udd34 Error accumulation: Mistakes at each step snowball for long horizons.</li> <li>\ud83d\udcc9 Weaker for long-range forecasts: Accuracy drops the further you predict.</li> </ul> <p></p> <p>When to use:</p> <ul> <li>Resource-constrained environments.</li> <li>Short-term forecasts or when model must update often.</li> </ul>"},{"location":"strategies/#3-hybrid-multi-step-forecasting","title":"3\ufe0f\u20e3 Hybrid Multi-step Forecasting","text":"<p>Definition: Combine direct and recursive: e.g., direct for first few steps, recursive after, or ensemble both.</p> <p>Advantages</p> <ul> <li>\ud83d\udd04 Balanced: Mixes accuracy with efficiency.</li> <li>\ud83e\udde9 Flexible: Can adapt to data/business need.</li> </ul> <p>Disadvantages</p> <ul> <li>\ud83e\uddd1\u200d\ud83d\udd2c Complex implementation: Logic and tuning more involved.</li> <li>\ud83d\udc77 May need custom code or frameworks.</li> </ul> <p></p> <p>When to use:</p> <ul> <li>When horizon is long and you want both accuracy and efficiency.</li> <li>Advanced pipelines or high-stakes business planning.</li> </ul>"},{"location":"strategies/#strategy-comparison","title":"\ud83d\udcca Strategy Comparison","text":"Strategy Model Count Error Propagation Customization Complexity Use Case \ud83d\udfe6 Direct High None Per-step High Max accuracy, long horizon \ud83d\udfe9 Recursive Low High Shared Low Simplicity, fast iteration \ud83d\udfe7 Hybrid Medium Medium Mixed Medium/High Best-of-both, advanced setups <p>Tip: Modern ML pipelines often allow you to choose or ensemble these strategies, so you get the best trade-off for your business case!</p>"},{"location":"strategies/#hierarchical-reconciliation-methods","title":"Hierarchical Reconciliation Methods","text":""},{"location":"strategies/#hierarchical-forecast-reconciliation","title":"Hierarchical Forecast Reconciliation","text":"<p>Hierarchy: Retail System \u2192 Area \u2192 Store \u2192 Product Group \u2192 SKU</p> <p>What is Forecast Reconciliation?</p> <p>In hierarchical time series forecasting, reconciliation means adjusting forecasts so that results at every level are consistent: the forecast for a higher level always equals the sum of forecasts at the levels below.</p> <pre><code>This guarantees that all reports and business decisions are based on numbers that \"add up\"\u2014from total system, to region, to store, to SKU.\n</code></pre>"},{"location":"strategies/#why-is-reconciliation-needed","title":"Why is Reconciliation Needed?","text":"<ul> <li>Prevents inconsistencies between total and detailed (store/SKU) forecasts.</li> <li>Ensures all stakeholders (from HQ to stores) can trust and align on the same numbers.</li> <li>Avoids planning and reporting confusion caused by non-coherent forecasts.</li> </ul>"},{"location":"strategies/#core-reconciliation-methods","title":"Core Reconciliation Methods","text":"Method How It Works Advantages Limitations Bottom-up Forecast at the lowest level (SKU), then aggregate up through the hierarchy Captures detailed local variation Sensitive to local noise and outliers Top-down Forecast at the highest level, then allocate down using ratios or rules Stable and efficient for total control Can miss store/SKU-level patterns and events Middle-out Forecast at a mid-level (e.g., Store), aggregate up and/or disaggregate down Balances granularity and aggregate accuracy Relies on mid-level data quality OLS Uses regression to minimize squared differences between levels Guarantees sum coherence; interpretable Assumes error variance is constant or linear MinTrace Uses error covariance to minimize total reconciliation variance Statistically optimal (lowest total variance) Computationally more demanding"},{"location":"strategies/#why-use-a-hybrid-approach","title":"Why Use a Hybrid Approach?","text":"<p>Why Hybrid?</p> <ul> <li>No single method is best for every dataset or business use case.</li> <li>By combining strategies, the system achieves:</li> <li>Consistency from total to lowest level</li> <li>Balance between detail and overall stability</li> <li>Adaptability to the business structure and priorities</li> </ul> <p>The pipeline can dynamically select and tune the reconciliation method each run\u2014often using:</p> <ul> <li>Middle-out as the main approach,</li> <li>Enhanced by Top-down, Bottom-up, and final adjustment using OLS and MinTrace when needed.</li> </ul>"},{"location":"strategies/#when-to-use-each-method","title":"When to Use Each Method","text":"Strategy How It Works Best Used When Bottom-up Aggregate lowest-level forecasts upward Data is detailed and high quality at the bottom Top-down Split high-level forecasts downward Macro-level targets, weaker data at detail level Middle-out Start at mid-level, adjust up and down Middle tiers are decision-driving OLS / MinTrace Adjustment Regression or covariance-based adjustment Deep hierarchies or strict audit requirements Hybrid (combined) Mix of all above, tuned per run Need both granularity and top-level accuracy <p>Business Impact</p> <p>Accurate, coherent forecasts empower inventory planning, sales targets, and resource allocation. Managers at every level\u2014from HQ to store\u2014see numbers that \u201cadd up\u201d, driving trust in the system.</p> <p>Considerations</p> <ul> <li>Bottom-up can amplify local noise.</li> <li>Top-down may overlook store or SKU-level changes.</li> <li>OLS/MinTrace adjustments require reliable error estimates for optimal results.</li> </ul>"},{"location":"strategies/#summary-table","title":"Summary Table","text":"Method Granularity Coherence Stability Complexity Best For Bottom-up High Yes Medium Low SKU-level or store-level control Top-down Low Yes High Low Corporate, strategic planning Middle-out Medium Yes Balanced Medium Multi-tier organizations OLS/MinTrace All Yes Highest High Deep hierarchies, compliance <p>A hybrid reconciliation approach ensures forecasts are accurate, robust, and actionable for every level of your business.</p> <p>For more details on implementation, see System Architecture</p>"},{"location":"tools/","title":"5. System Architecture","text":""},{"location":"tools/#modeling-feature-engineering-toolkit","title":"Modeling &amp; Feature Engineering Toolkit","text":"Tool / Library Purpose &amp; Highlights Scikit-learn Essential Python library for machine learning, supporting a wide range of supervised and unsupervised algorithms, as well as data preprocessing and model evaluation. Optuna Automated hyperparameter optimization framework; efficiently searches for optimal configurations through adaptive and distributed experimentation. TSFresh Automates the extraction of a large number of time series features, enabling advanced signal processing and feature engineering for temporal data. SHAP Delivers model interpretability via Shapley values; quantifies the contribution of each feature to individual predictions for robust explainability and auditing. HierarchicalForecast Specialized library for forecasting hierarchical time series, ensuring coherence and consistency across all levels of data aggregation (e.g., national, regional, store, SKU)."},{"location":"tools/#mlops-dataops-productionization","title":"MLOps, DataOps &amp; Productionization","text":"<ul> <li>DVC: Data and pipeline versioning for full reproducibility and collaboration</li> <li>MLflow: Experiment and model tracking, registry, and deployment</li> <li>Apache Airflow / Dagster: Workflow orchestration for ETL, training, evaluation, and reporting pipelines</li> <li>FastAPI: High-performance, production-ready API serving for real-time inference</li> <li>Streamlit: Business Intelligence dashboards for interactive analytics and visual monitoring</li> <li>Optuna: Integrated with pipelines for scalable, automated hyperparameter search</li> <li>Slack (or alert system): Notifications, anomaly detection, and workflow monitoring</li> </ul> Tool / Component Key Role in Production Workflow Streamlit Interactive dashboards for operational monitoring, real-time visualization of trends, model outputs, and error analysis. Connects live to APIs or batch outputs. DVC Data and pipeline version control; tracks every change in datasets, model artifacts, and intermediate files. Enables reproducibility and seamless team collaboration. MLflow End-to-end ML lifecycle management. Tracks experiments, metrics, hyperparameters, and model versions; supports model registry and REST API/batch serving. Dagster Modern orchestrator for building, scheduling, and monitoring complex data and ML workflows. Ensures reliability and observability in production pipelines. Apache Airflow Industry-standard DAG-based orchestrator. Automates ETL, model training, evaluation, and scheduled inference. Enables robust error handling and pipeline reusability. <p>This technology stack ensures high scalability, reliability, and full traceability for all machine learning and data operations\u2014from data ingestion and feature engineering to model deployment and business reporting.</p>"}]}